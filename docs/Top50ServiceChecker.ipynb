{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf07deca",
   "metadata": {},
   "source": [
    "# Top 50/75 Service Checker\n",
    "A 22.4 hackfest projects by some manager types who still like to play\n",
    "\n",
    "Goal:\n",
    "* Dashboard the level of services available for our Top 50/75 datasets (limited to those discoverable via CMR)\n",
    "\n",
    "Strech Goals:\n",
    "* quantify (and visualize) the problem of “I have to do something different for different datasets every time I access/read/plot”\n",
    "* if we have x services on x datasets - can we search/access/read/viz them all the same way across those different endpoints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef2827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install prerequisite packages\n",
    "import sys\n",
    "\n",
    "# Note you usually need to install gdal outside of Python / pip first. On OSX, brew install gdal\n",
    "!{sys.executable} -m pip install rasterio s3fs OWSLib GDAL matplotlib netCDF4 numpy xarray h5netcdf hvplot plotly seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f10ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib import request, parse\n",
    "from http.cookiejar import CookieJar\n",
    "import getpass\n",
    "import netrc\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import s3fs\n",
    "import netrc\n",
    "import json\n",
    "import pandas as pd\n",
    "from osgeo import gdal\n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns\n",
    "from difflib import SequenceMatcher\n",
    "#import holoviews as hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 50 / Stretch 75 Collection Shortnames\n",
    "\n",
    "top_datasets = {\n",
    "\"SENTINEL-1A_SLC\",\n",
    "\"GPM_3IMERGHH\",\n",
    "\"NLDAS_FORA0125_H\",\n",
    "\"M2T1NXSLV\",\n",
    "\"GLDAS_NOAH025_3H\",\n",
    "\"GPM_3IMERGHHL\",\n",
    "\"GPM_3IMERGHHE\",\n",
    "\"NLDAS_NOAH0125_H\",\n",
    "\"M2I3NPASM\",\n",
    "\"M2I1NXASM\",\n",
    "\"GPM_3IMERGDF\",\n",
    "\"M2T1NXFLX\",\n",
    "\"M2T1NXRAD\",\n",
    "\"NLDAS_MOS0125_H\",\n",
    "\"GPM_MERGIR\",\n",
    "\"NLDAS_VIC0125_H\",\n",
    "\"GLDAS_NOAH10_3H\",\n",
    "\"S5P_L2__NO2___\",\n",
    "\"S5P_L2__CO____\",\n",
    "\"S5P_L2__CH4___\",\n",
    "\"S5P_L2__SO2___\",\n",
    "\"MOD04_L2\",\n",
    "\"MYD04_L2\",\n",
    "\"VNP46A2\",\n",
    "\"MYD03\",\n",
    "\"MYD021KM\",\n",
    "\"MYD06_L2\",\n",
    "\"MYD35_L2\",\n",
    "\"MOD03\",\n",
    "\"MOD021KM\",\n",
    "\"MOD06_L2\",\n",
    "\"VNP46A1\",\n",
    "\"MOD07_L2\",\n",
    "\"MOD05_L2\",\n",
    "\"MOD02HKM\",\n",
    "\"MCD06COSP_D3_MODIS\",\n",
    "\"MYD04_3K\",\n",
    "\"MYD07_L2\",\n",
    "\"MOD04_3K\",\n",
    "\"MOD35_L2\",\n",
    "\"MYD05_L2\",\n",
    "\"MOD11A1\",\n",
    "\"MYD11A1\",\n",
    "\"MOD11A2\",\n",
    "\"MYD11A2\",\n",
    "\"MCD19A2\",\n",
    "\"MYD09GA\",\n",
    "\"MOD15A2H\",\n",
    "\"MOD09GA\",\n",
    "\"MCD43A4\",\n",
    "\"MOD14\",\n",
    "\"MCD15A2H\",\n",
    "\"MYD14\",\n",
    "\"MCD43A3\",\n",
    "\"MOD13Q1\",\n",
    "\"MYD13Q1\",\n",
    "\"MOD13A3\",\n",
    "\"MYD13A3\",\n",
    "\"MYD09GQ\",\n",
    "\"MCD15A3H\",\n",
    "\"ASTGTM\",\n",
    "\"ASTGTM_NC\",\n",
    "\"MOD09GQ\",\n",
    "\"MOD09A1\",\n",
    "\"MOD16A2\",\n",
    "\"MOD09Q1\",\n",
    "\"MYD09Q1\",\n",
    "\"MYD09A1\",\n",
    "\"MOD11_L2\",\n",
    "\"MYD11_L2\",\n",
    "\"DAYMET_DAILY_V4_1840\",\n",
    "\"PODAAC-GHMDA-2PJ19\",\n",
    "\"PODAAC-OSCAR-03D01\",\n",
    "\"PODAAC-GHGMR-4FJ04\",\n",
    "\"VIIRS_NPP-STAR-L3U-v2.80\"\n",
    "}\n",
    "\n",
    "provider_list = {\n",
    "    \"ASF\", \"GES_DISC\", \"LAADS\", \"LPCLOUD\", \"POCLOUD\", \"GHRC_DAAC\", \"ORNL_CLOUD\", \"NSIDC_CPRD\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scorer method which looks at each dataset and based on its characteristics, gives it both an 'ideal' and 'actual' score to assess how compliant is it with the Levels of Service\n",
    "#This is overly simplistic!\n",
    "\n",
    "#Background on the has<...> fields is here: https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html\n",
    "\n",
    "\n",
    "def scorer (df_to_score):\n",
    "    actual_score = 0 \n",
    "    ideal_score = 0\n",
    "    \n",
    "    #calculate ideal score\n",
    "    if (df_to_score.loc[0,'nativeDataFormats'] is not None and len(df_to_score.loc[0,'nativeDataFormats']) > 0):\n",
    "        for format in (df_to_score.loc[0,'nativeDataFormats']):\n",
    "            if (format.find(\"DF\") != -1):\n",
    "                #this is something like HDF/NetCDF\n",
    "                ideal_score = 4 #1 point each for reformatting, spatial, temporal subetting or 'other' which could be reprojection (I can't check variable subsetting :( )\n",
    "        \n",
    "    #calculate actual score\n",
    "    if (df_to_score.loc[0,'hasSpatialSubsetting'] == True):\n",
    "        actual_score = actual_score + 1\n",
    "        \n",
    "    if (df_to_score.loc[0,'hasTemporalSubsetting']== True):\n",
    "        actual_score = actual_score + 1\n",
    "        \n",
    "    if (df_to_score.loc[0,'hasFormats'] == True):\n",
    "        actual_score = actual_score + 1\n",
    "        \n",
    "    if (df_to_score.loc[0,'hasTransforms'] == True):\n",
    "        actual_score = actual_score + 1\n",
    "        \n",
    "    #print(df_to_score)\n",
    "    df_to_score = df_to_score.assign(idealScore=[ideal_score])\n",
    "    df_to_score = df_to_score.assign(actualScore=[actual_score])\n",
    "    \n",
    "    return df_to_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't use pandas well with the native data formats since they are a list, so this yanks them out into their own columns.  Not perfect, but better than nothing.\n",
    "def dataFormatCleaner(df_to_clean):\n",
    "    #num_formats = 1\n",
    "    \n",
    "    if (df_to_clean.loc[0,'nativeDataFormats'] is not None and len(df_to_clean.loc[0,'nativeDataFormats']) > 0):\n",
    "        for format in (df_to_clean.loc[0,'nativeDataFormats']):\n",
    "            #trying to make a dynamic column name on the fly, but it wasn't working.  For another time.\n",
    "            #column_name_string = \"nativeDataFormat\"+str(num_formats)+\"=[\"+format+\"]\"\n",
    "            df_to_clean = df_to_clean.assign(nativeDataFormat1=[format])\n",
    "            break\n",
    "            #num_formats = num_formats + 1\n",
    "            \n",
    "    return df_to_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40accc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query graphQL\n",
    "\n",
    "def queryGraphQL (filter_str):\n",
    "    graphql_rooturl = \"https://graphql.earthdata.nasa.gov/api\"\n",
    "    query = \"\"\"\n",
    "    query{\n",
    "    collections (cloudHosted: true, limit: 2000, %s ){\n",
    "        count\n",
    "        items {\n",
    "          provider\n",
    "          shortName\n",
    "          conceptId\n",
    "          nativeDataFormats\n",
    "          processingLevel\n",
    "          hasFormats\n",
    "          hasSpatialSubsetting\n",
    "          hasTemporalSubsetting\n",
    "          hasTransforms\n",
    "          services {\n",
    "            count\n",
    "            items {\n",
    "              type\n",
    "            }\n",
    "          }\n",
    "          granules {\n",
    "            count\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\" % (filter_str)\n",
    "\n",
    "    response = requests.post(url=graphql_rooturl, json={\"query\": query})\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error with \", filter_str)\n",
    "        print(\"response status code: \", response.status_code)\n",
    "        print(\"response : \", response.content)\n",
    "    \n",
    "\n",
    "    # Shove it into a dataframe\n",
    "    json_data = json.loads(response.text)\n",
    "    df_data = json_data['data']['collections']['items']\n",
    "    new_df = pd.json_normalize(df_data)\n",
    "    \n",
    "    return new_df;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through Top 50/75 datasets, query, score and add to dataframe\n",
    "\n",
    "topdata_df = pd.DataFrame()\n",
    "count = 0\n",
    "\n",
    "for shortname in (top_datasets):\n",
    "    #print(shortname)\n",
    "    #if (count <=3):\n",
    "    filter_str = \"shortName: \\\"\" + shortname + \"\\\"\"\n",
    "    new_df = queryGraphQL(filter_str)\n",
    "    \n",
    "    #score this\n",
    "    if (len(new_df) == 0):\n",
    "        print(shortname + \" has no matching data\")\n",
    "    else:\n",
    "        if (len(new_df) > 1):\n",
    "            print(shortname + \" has more than 1 entry, bail for now\")      \n",
    "        else:\n",
    "            new_df = scorer(new_df)\n",
    "            new_df = dataFormatCleaner(new_df)\n",
    "            topdata_df = pd.concat([topdata_df, new_df], ignore_index=True)\n",
    "        #count = count + 1\n",
    "\n",
    "display(topdata_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_score = topdata_df.groupby(['processingLevel.id'])['actualScore'].sum()\n",
    "ideal_score = topdata_df.groupby(['processingLevel.id'])['idealScore'].sum()\n",
    "\n",
    "top_merged = pd.DataFrame()\n",
    "top_merged['actualScore'] = actual_score\n",
    "top_merged['idealScore'] = ideal_score\n",
    "\n",
    "\n",
    "top_merged.plot(y=[\"actualScore\", \"idealScore\"], use_index=True, kind=\"bar\", title=\"Top 50/75 - Levels of Service\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55012e4c",
   "metadata": {},
   "source": [
    "# Try all the cloud hosted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a930610",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_the_data_df = pd.DataFrame()\n",
    "\n",
    "for provider in (provider_list):\n",
    "    print(provider)\n",
    "    filter_str = \"provider: \\\"\" + provider + \"\\\"\"\n",
    "    new_df = queryGraphQL(filter_str)\n",
    "        \n",
    "    #score this\n",
    "    if (len(new_df) == 0):\n",
    "        print(provider + \" has no matching data\")\n",
    "    else:\n",
    "        for index, row in new_df.iterrows():\n",
    "            transposed = row.to_frame().T.reset_index()\n",
    "            scored_df = scorer(transposed)\n",
    "            scored_df = dataFormatCleaner(scored_df)\n",
    "                \n",
    "            #add new data to the full table\n",
    "            all_the_data_df = pd.concat([all_the_data_df, scored_df], ignore_index=True)\n",
    "        \n",
    "display(all_the_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c553ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_score = all_the_data_df.groupby(['processingLevel.id'])['actualScore'].sum()\n",
    "ideal_score = all_the_data_df.groupby(['processingLevel.id'])['idealScore'].sum()\n",
    "\n",
    "merged = pd.DataFrame()\n",
    "merged['actualScore'] = actual_score\n",
    "merged['idealScore'] = ideal_score\n",
    "\n",
    "merged.plot(y=[\"actualScore\", \"idealScore\"], use_index=True, kind=\"bar\", title=\"All Cloud Data - Levels of Service\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dbad64",
   "metadata": {},
   "source": [
    "# Identify areas of opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for biggest areas to focus on within the Top 50/75 data:\n",
    "\n",
    "opportunities = all_the_data_df.groupby(['processingLevel.id','nativeDataFormat1']).size()\n",
    "opportunities = opportunities.unstack()\n",
    "opportunities_t = opportunities.T\n",
    "plt.figure(figsize=(10,40))\n",
    "sns.heatmap(opportunities_t, vmin=10, vmax=200, cmap=\"coolwarm\", annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36bfe96",
   "metadata": {},
   "source": [
    "\n",
    "# Service Gurus - phone a friend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7252be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for gurus to talk to (DAACs who have wired up at least 1 service in their cloud data)\n",
    "service_gurus = all_the_data_df.loc[all_the_data_df['services.count'] >= 1]\n",
    "\n",
    "display(service_gurus.groupby(['provider']).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a73dc5",
   "metadata": {},
   "source": [
    "# While at it, make our other metric generation super easy too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "collectionCounts = all_the_data_df.groupby(['provider']).size()\n",
    "granuleCounts = all_the_data_df.groupby(['provider'])['granules.count'].sum()\n",
    "\n",
    "merged = pd.DataFrame()\n",
    "merged['granuleCount'] = granuleCounts\n",
    "merged['collectionCount']= collectionCounts\n",
    "\n",
    "display(merged)\n",
    "\n",
    "merged.plot(y=[\"granuleCount\"], use_index=True, kind=\"bar\", title=\"All Cloud Data - Granules per Provider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c9e42",
   "metadata": {},
   "source": [
    "Things we learned:\n",
    "* 5 of the top 75 are no longer available?\n",
    "* 2 of the top 75 have 2 entries with the same shortname?\n",
    "* Our metadata is still _really_ inconsistent.  Data formats entirely missing, processing levels not standardized\n",
    "* It'd be helpful to easily query on 'hasOPeNDAP' (Amy's working this PR)\n",
    "* It'd be helpful to easily query on 'hasVariableSubetting' and 'hasReprojection'\n",
    "* Pandas makes a lot of things really, really easy.  But, the graph nature of our metadata means that we hit limitations with just using panda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97f608",
   "metadata": {},
   "source": [
    "# For future playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3627dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start with services first -> Collections\n",
    "def queryGraphQLforServices ():\n",
    "    graphql_rooturl = \"https://graphql.earthdata.nasa.gov/api\"\n",
    "    \n",
    "            \n",
    "    services_query = \"\"\"\n",
    "        query{\n",
    "            services(limit: 2000) {\n",
    "              items {\n",
    "                type\n",
    "                conceptId\n",
    "                name\n",
    "                serviceOptions\n",
    "                supportedInputProjections\n",
    "                supportedOutputProjections\n",
    "                supportedReformattings\n",
    "                collections() {\n",
    "                  count\n",
    "                  items {\n",
    "                    conceptId\n",
    "                    cloudHosted\n",
    "                  }\n",
    "                }\n",
    "\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(url=graphql_rooturl, json={\"query\": services_query})\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"response status code: \", response.status_code)\n",
    "        print(\"response : \", response.content)\n",
    "        print(\"request: \", response.request)\n",
    "    \n",
    "    \n",
    "    # Shove it into a dataframe\n",
    "    json_data = json.loads(response.text)\n",
    "    df_data = json_data['data']['services']['items']\n",
    "    new_df = pd.json_normalize(df_data)\n",
    "    \n",
    "    return new_df;\n",
    "    \n",
    "\n",
    "services_df = queryGraphQLforServices()\n",
    "display(services_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b342db-7d99-4f13-b1b1-f1e56b9ab965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
